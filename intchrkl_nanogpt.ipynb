{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# NanoGPT Notebook\n",
        "\n",
        "Based on Andrej Karpathy's NanoGPT, with elements from \"Attention Is All You Need\"\n",
        "\n",
        "References:\n",
        "\n",
        "\n",
        "\n",
        "*   [Karpathy](https://www.youtube.com/watch?v=kCc8FmEb1nY&ab_channel=AndrejKarpathy)\n",
        "*   Vaswani, Ashish, et al. \"Attention is all you need.\" Advances in neural information processing systems 30 (2017).\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PaZGezinNEsI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the cell below to upload a training dataset. I used the Tiny Shakespeare [corpus](https://github.com/intchrkl/nanogpt/blob/main/input.txt)."
      ],
      "metadata": {
        "id": "fQm4_uk3NyLS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "eNnuFDsjdVCE",
        "outputId": "078c2ea9-ad44-4d46-feec-f3e28bde6fc1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ff352d6e-1bbe-4807-9781-3e0f9bab0c23\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ff352d6e-1bbe-4807-9781-3e0f9bab0c23\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving input.txt to input (1).txt\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the cell below to load the model and helpers."
      ],
      "metadata": {
        "id": "-nNQKfwZODUJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "p0XQT6gAietH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "from torch.nn import functional as F\n",
        "\n",
        "def get_model_and_optimizer(config, vocab_size):\n",
        "    class Head(nn.Module):\n",
        "        # A self-attention head\n",
        "\n",
        "        def __init__(self, head_size):\n",
        "            super().__init__()\n",
        "            self.key = nn.Linear(config['n_embd'], head_size, bias=False)\n",
        "            self.query = nn.Linear(config['n_embd'], head_size, bias=False)\n",
        "            self.value = nn.Linear(config['n_embd'], head_size, bias=False)\n",
        "            self.register_buffer('tril', torch.tril(torch.ones(config['block_size'], config['block_size'])))\n",
        "            self.dropout = nn.Dropout(config['dropout'])\n",
        "\n",
        "        def forward(self, x):\n",
        "            B, T, C = x.shape\n",
        "            k = self.key(x) # (B, T, C)\n",
        "            q = self.query(x) # (B, T, C)\n",
        "\n",
        "            # From Attention Is All You Need paper:\n",
        "            wgt = q @ k.transpose(-2, -1) / math.sqrt(C) # QK^T/sqrt(d_k)\n",
        "            wgt = wgt.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "            wgt = F.softmax(wgt, dim=-1)\n",
        "            wgt = self.dropout(wgt)\n",
        "\n",
        "            v = self.value(x)\n",
        "            out = wgt @ v\n",
        "            return out\n",
        "\n",
        "    class MultiHeadAttention(nn.Module):\n",
        "        # A collection of self-attention heads in parallel\n",
        "\n",
        "        def __init__(self, num_heads, head_size):\n",
        "            super().__init__()\n",
        "            # Multiple heads in parallel\n",
        "            heads_list = [Head(head_size) for _ in range(num_heads)]\n",
        "            self.heads = nn.ModuleList(heads_list)\n",
        "            self.proj = nn.Linear(num_heads * head_size, config['n_embd'])\n",
        "            self.dropout = nn.Dropout(config['dropout'])\n",
        "\n",
        "\n",
        "        def forward(self, x):\n",
        "            out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "            out = self.dropout(self.proj(out))\n",
        "            return out\n",
        "\n",
        "    class FeedForward(nn.Module):\n",
        "\n",
        "        def __init__(self, n_embd):\n",
        "            super().__init__()\n",
        "            self.net = nn.Sequential(\n",
        "                nn.Linear(n_embd, 4 * n_embd),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(4 * n_embd, n_embd),\n",
        "                nn.Dropout(config['dropout'])\n",
        "            )\n",
        "\n",
        "        def forward(self, x):\n",
        "            return self.net(x)\n",
        "\n",
        "    class Block(nn.Module):\n",
        "\n",
        "        def __init__(self, n_embd, n_heads):\n",
        "            super().__init__()\n",
        "            head_size = n_embd // n_heads\n",
        "            self.self_attn = MultiHeadAttention(n_heads, head_size)\n",
        "            self.feed_fwd = FeedForward(n_embd)\n",
        "            self.ln1 = nn.LayerNorm(n_embd)\n",
        "            self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = x + self.self_attn(self.ln1(x))\n",
        "            x = x + self.feed_fwd(self.ln2(x))\n",
        "            return x\n",
        "\n",
        "    class GPT(nn.Module):\n",
        "\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "            self.token_embedding_table = nn.Embedding(vocab_size, config['n_embd'])\n",
        "            self.position_embedding_table = nn.Embedding(config['block_size'], config['n_embd'])\n",
        "            self.blocks = nn.Sequential(*[Block(config['n_embd'], config['n_head']) for _ in range(config['n_layer'])])\n",
        "            self.ln_final = nn.LayerNorm(config['n_embd'])\n",
        "            self.lm_head = nn.Linear(config['n_embd'], vocab_size)\n",
        "\n",
        "        def forward(self, idx, targets=None):\n",
        "            B, T = idx.shape\n",
        "\n",
        "            tok_embd = self.token_embedding_table(idx)\n",
        "            pos_embd = self.position_embedding_table(torch.arange(T, device=config['device']))\n",
        "\n",
        "            x = tok_embd + pos_embd\n",
        "            x = self.blocks(x)\n",
        "            x = self.ln_final(x)\n",
        "            logits = self.lm_head(x)\n",
        "\n",
        "            if targets is None:\n",
        "                loss = None\n",
        "            else:\n",
        "                B, T, C = logits.shape\n",
        "                logits = logits.view(B * T, C)\n",
        "                targets = targets.view(B * T)\n",
        "                loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "            return logits, loss\n",
        "\n",
        "        def generate(self, idx, max_new_tokens):\n",
        "            for _ in range(max_new_tokens):\n",
        "                idx_and_context = idx[:, -config['block_size']:]\n",
        "                logits, _ = self.forward(idx_and_context)\n",
        "                logits = logits[:, -1, :]\n",
        "                probs = F.softmax(logits, dim=-1)\n",
        "                idx_next = torch.multinomial(probs, num_samples=1)\n",
        "                idx = torch.cat((idx, idx_next), dim=1)\n",
        "            return idx\n",
        "\n",
        "    model = GPT().to(config['device'])\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=config['learning_rate'])\n",
        "    return model, optimizer\n",
        "\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "# Simple encoding for each letter (token)\n",
        "char_to_int = { c:i for i, c in enumerate(chars)}\n",
        "int_to_char = { i:c for i, c in enumerate(chars)}\n",
        "\n",
        "# Encode a string S\n",
        "def encode(S):\n",
        "    return [char_to_int[c] for c in S]\n",
        "\n",
        "# Decode a list of integers L\n",
        "def decode(L):\n",
        "    return ''.join([int_to_char[i] for i in L])\n",
        "\n",
        "# Encoding the entire corpus and storing it in a Tensor\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "\n",
        "# Partition data into train and validation sets\n",
        "percent_train = 0.9 # portion of data to be used as training set, rest is val\n",
        "train_data = data[:int(0.9 * len(data))]\n",
        "val_data = data[int(0.9 * len(data)):]\n",
        "\n",
        "# Samples a mini-batch of sequence from either the training or validation set\n",
        "def get_batch(split, config):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    start_idxs = torch.randint(0, len(data) - config['block_size'], (config['batch_size'],))\n",
        "    input_tensors = torch.stack([data[i:i + config['block_size']] for i in start_idxs])\n",
        "    target_tensors = torch.stack([data[i + 1:i + config['block_size'] + 1] for i in start_idxs])\n",
        "    return input_tensors.to(config['device']), target_tensors.to(config['device'])\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss(model, config):\n",
        "    model.eval()\n",
        "    losses = {'train': 0, 'val': 0}\n",
        "    for split in ['train', 'val']:\n",
        "        loss_total = 0\n",
        "        for _ in range(config['eval_iters']):\n",
        "            xb, yb = get_batch(split, config)\n",
        "            _, loss = model(xb, yb)\n",
        "            loss_total += loss.item()\n",
        "        losses[split] = loss_total / config['eval_iters']\n",
        "    model.train()\n",
        "    return losses\n",
        "\n",
        "def train(model, optimizer, config):\n",
        "  for step in range(config['max_iters']):\n",
        "    if step % config['eval_interval'] == 0:\n",
        "        losses = estimate_loss(model, config)\n",
        "        print(f\"Step {step}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    xb, yb = get_batch('train', config)\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  print(\"Training completed.\")\n",
        "\n",
        "def parse_output(generated):\n",
        "  return decode(generated[0].tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the cell below to train using the hyperparameters specified in `config`"
      ],
      "metadata": {
        "id": "q1je2dZ1OPR9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    'batch_size' : 16, # no. of independent sequences to process in parallel\n",
        "    'block_size' : 32, # max no. of tokens to use for context in predictions\n",
        "    'max_iters' : 5000,\n",
        "    'eval_iters' : 200,\n",
        "    'eval_interval' : 100,\n",
        "    'learning_rate' : 1e-3,\n",
        "    'n_embd' : 64,\n",
        "    'n_head' : 4,\n",
        "    'n_layer' : 4,\n",
        "    'dropout' : 0.0,\n",
        "    'device' : 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "    'seed': 18916\n",
        "}\n",
        "\n",
        "torch.manual_seed(config['seed'])\n",
        "model, optimizer = get_model_and_optimizer(config, vocab_size)\n",
        "train(model, optimizer, config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYzEDsH9KovA",
        "outputId": "9666d041-5cd4-4c9b-e4a1-cdba41848ea3"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0: train loss 4.3885, val loss 4.3838\n",
            "Step 100: train loss 2.6421, val loss 2.6538\n",
            "Step 200: train loss 2.5216, val loss 2.5563\n",
            "Step 300: train loss 2.4412, val loss 2.4612\n",
            "Step 400: train loss 2.3608, val loss 2.3685\n",
            "Step 500: train loss 2.3232, val loss 2.3375\n",
            "Step 600: train loss 2.2529, val loss 2.2728\n",
            "Step 700: train loss 2.2138, val loss 2.2312\n",
            "Step 800: train loss 2.1830, val loss 2.2324\n",
            "Step 900: train loss 2.1269, val loss 2.1610\n",
            "Step 1000: train loss 2.1065, val loss 2.1292\n",
            "Step 1100: train loss 2.0682, val loss 2.1196\n",
            "Step 1200: train loss 2.0474, val loss 2.0977\n",
            "Step 1300: train loss 2.0248, val loss 2.0710\n",
            "Step 1400: train loss 1.9929, val loss 2.0548\n",
            "Step 1500: train loss 1.9700, val loss 2.0378\n",
            "Step 1600: train loss 1.9457, val loss 2.0304\n",
            "Step 1700: train loss 1.9334, val loss 2.0108\n",
            "Step 1800: train loss 1.9188, val loss 2.0038\n",
            "Step 1900: train loss 1.9048, val loss 1.9874\n",
            "Step 2000: train loss 1.8856, val loss 1.9782\n",
            "Step 2100: train loss 1.8704, val loss 1.9792\n",
            "Step 2200: train loss 1.8548, val loss 1.9729\n",
            "Step 2300: train loss 1.8409, val loss 1.9560\n",
            "Step 2400: train loss 1.8321, val loss 1.9419\n",
            "Step 2500: train loss 1.8240, val loss 1.9365\n",
            "Step 2600: train loss 1.8021, val loss 1.9333\n",
            "Step 2700: train loss 1.7956, val loss 1.9292\n",
            "Step 2800: train loss 1.7884, val loss 1.9185\n",
            "Step 2900: train loss 1.7677, val loss 1.8989\n",
            "Step 3000: train loss 1.7608, val loss 1.9016\n",
            "Step 3100: train loss 1.7691, val loss 1.8819\n",
            "Step 3200: train loss 1.7557, val loss 1.8775\n",
            "Step 3300: train loss 1.7422, val loss 1.8817\n",
            "Step 3400: train loss 1.7280, val loss 1.8683\n",
            "Step 3500: train loss 1.7279, val loss 1.8692\n",
            "Step 3600: train loss 1.7283, val loss 1.8477\n",
            "Step 3700: train loss 1.7210, val loss 1.8528\n",
            "Step 3800: train loss 1.7185, val loss 1.8481\n",
            "Step 3900: train loss 1.7168, val loss 1.8585\n",
            "Step 4000: train loss 1.6927, val loss 1.8439\n",
            "Step 4100: train loss 1.6898, val loss 1.8303\n",
            "Step 4200: train loss 1.6901, val loss 1.8460\n",
            "Step 4300: train loss 1.6851, val loss 1.8255\n",
            "Step 4400: train loss 1.6834, val loss 1.8418\n",
            "Step 4500: train loss 1.6671, val loss 1.8182\n",
            "Step 4600: train loss 1.6777, val loss 1.8407\n",
            "Step 4700: train loss 1.6599, val loss 1.8103\n",
            "Step 4800: train loss 1.6610, val loss 1.8036\n",
            "Step 4900: train loss 1.6677, val loss 1.8146\n",
            "Training completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model converges quite well, but we can try to tune the hyperparameters even further."
      ],
      "metadata": {
        "id": "NhReRDnQQazO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "View the output \\(first 2000 tokens\\) generated by the model.\n"
      ],
      "metadata": {
        "id": "lbxNIhKWOXQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context = torch.zeros((1, 1), dtype=torch.long, device=config['device'])\n",
        "generated = model.generate(context, max_new_tokens=2000)\n",
        "print(parse_output(generated))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-0d_5PbK5dI",
        "outputId": "c589379e-686b-495e-c321-0cdac3c2c18a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Is now to is we his brother to to mady ques heaven?\n",
            "\n",
            "That have him mader:\n",
            "Were the dold, Looth, be it!\n",
            "Made, here on yours, as Stear him some that Dead,\n",
            "Befitedil now to that is fortusing dreath,\n",
            "To her honour thoight\n",
            "By for of branished minen master.\n",
            "\n",
            "FRIAR MARDINTA:\n",
            "Not is servines at I was be ven light the grace,\n",
            "Thate to burn the madaber to mind.\n",
            "Yeas, Farewell hence befores, of my wound kind\n",
            "Now, my love lies our forrow heavens.\n",
            "\n",
            "StOLANNES:\n",
            "What,--'is well\n",
            "Stens your on will bidt Bariemy swaget,\n",
            "Mareth, being fark i counteny taken,\n",
            "Of with daless think his promieve any and that\n",
            "yet abours I aw KI's now will is assencharvence,\n",
            "These to been affer's teart,'d weepon mised;\n",
            "Before obgatingn Dry, balieve aliancisent the maides,\n",
            "here on to there-seath affurnc'd I colforce.\n",
            "\n",
            "CAMILLO:\n",
            "Here safe it.\n",
            "\n",
            "StREN MERVOLI:\n",
            "Mastish, yet, beshand, kings, all,\n",
            "This abouther evill: they sispretaking from worshiold, brog dy ttio miseece we love:\n",
            "I do sland to caught bloteres, Spored,\n",
            "To me atort man king hase an\n",
            "Whose, stake wen most we mand takers dowered will from in our affils,\n",
            "What's o fimazen that Edwings to more\n",
            "To my arcish, he been your sorry issual.\n",
            "I would beer: you remy, so not thee,\n",
            "Ah? sir him thy talk of good the defeted.\n",
            "\n",
            "CARINCE ELLO:\n",
            "Tell Caric, I to staret, Main Told have shall'd,\n",
            "Befes thy flieldd about, Cliffiet if my butly your haven;\n",
            "Bnie, your doperli's your some afieured: take peart,\n",
            "Piris, be madan as sheeters, he gibk my hate for comeo,\n",
            "What the gace with this had! you, and were against on\n",
            "And to a manshappy's like;\n",
            "Our itteed a yield otcleep albard mele have moughty insmot\n",
            "the stience in skill arm First agring in mores ifsecrice\n",
            "then majuded again, Cliffer, that thous verse you spire heaven,\n",
            "Lay you verends her mreasong to to-say,\n",
            "Would we bing that home! 'All this like, if Claret then eut will, stark be the from\n",
            "Have and mean agre, now, moreful to to jespies very,\n",
            "Then that our give beer is grims if done?\n",
            "\n",
            "PETRGHAM:\n",
            "Were it; them let's as fearl! Gover\n",
            "Tha\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Increasing the `block_size` from `32` to `256` to give the model more context when predicting tokens."
      ],
      "metadata": {
        "id": "Rl7DjBDoQ7I0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    'batch_size' : 16, # no. of independent sequences to process in parallel\n",
        "    'block_size' : 256, # max no. of tokens to use for context in predictions\n",
        "    'max_iters' : 5000,\n",
        "    'eval_iters' : 200,\n",
        "    'eval_interval' : 100,\n",
        "    'learning_rate' : 1e-3,\n",
        "    'n_embd' : 64,\n",
        "    'n_head' : 4,\n",
        "    'n_layer' : 4,\n",
        "    'dropout' : 0.0,\n",
        "    'device' : 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "    'seed': 18916\n",
        "}\n",
        "\n",
        "torch.manual_seed(config['seed'])\n",
        "model, optimizer = get_model_and_optimizer(config, vocab_size)\n",
        "train(model, optimizer, config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bg1SgDemQ58z",
        "outputId": "35644316-8d41-46e4-9b0f-607f49c5713e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0: train loss 4.3071, val loss 4.3002\n",
            "Step 100: train loss 2.6442, val loss 2.6537\n",
            "Step 200: train loss 2.5335, val loss 2.5395\n",
            "Step 300: train loss 2.4934, val loss 2.5009\n",
            "Step 400: train loss 2.4637, val loss 2.4804\n",
            "Step 500: train loss 2.4314, val loss 2.4455\n",
            "Step 600: train loss 2.3847, val loss 2.3958\n",
            "Step 700: train loss 2.3186, val loss 2.3406\n",
            "Step 800: train loss 2.2490, val loss 2.2689\n",
            "Step 900: train loss 2.2053, val loss 2.2406\n",
            "Step 1000: train loss 2.1573, val loss 2.1956\n",
            "Step 1100: train loss 2.1159, val loss 2.1609\n",
            "Step 1200: train loss 2.0722, val loss 2.1300\n",
            "Step 1300: train loss 2.0301, val loss 2.0987\n",
            "Step 1400: train loss 1.9958, val loss 2.0764\n",
            "Step 1500: train loss 1.9672, val loss 2.0494\n",
            "Step 1600: train loss 1.9395, val loss 2.0274\n",
            "Step 1700: train loss 1.9042, val loss 2.0042\n",
            "Step 1800: train loss 1.8754, val loss 1.9772\n",
            "Step 1900: train loss 1.8524, val loss 1.9644\n",
            "Step 2000: train loss 1.8238, val loss 1.9437\n",
            "Step 2100: train loss 1.8044, val loss 1.9249\n",
            "Step 2200: train loss 1.7808, val loss 1.9035\n",
            "Step 2300: train loss 1.7656, val loss 1.9018\n",
            "Step 2400: train loss 1.7578, val loss 1.8985\n",
            "Step 2500: train loss 1.7423, val loss 1.8781\n",
            "Step 2600: train loss 1.7086, val loss 1.8622\n",
            "Step 2700: train loss 1.7159, val loss 1.8576\n",
            "Step 2800: train loss 1.6917, val loss 1.8468\n",
            "Step 2900: train loss 1.6756, val loss 1.8346\n",
            "Step 3000: train loss 1.6645, val loss 1.8305\n",
            "Step 3100: train loss 1.6627, val loss 1.8294\n",
            "Step 3200: train loss 1.6451, val loss 1.8209\n",
            "Step 3300: train loss 1.6368, val loss 1.8073\n",
            "Step 3400: train loss 1.6281, val loss 1.7970\n",
            "Step 3500: train loss 1.6214, val loss 1.7911\n",
            "Step 3600: train loss 1.6127, val loss 1.7889\n",
            "Step 3700: train loss 1.6131, val loss 1.7862\n",
            "Step 3800: train loss 1.6009, val loss 1.7768\n",
            "Step 3900: train loss 1.5915, val loss 1.7787\n",
            "Step 4000: train loss 1.5892, val loss 1.7782\n",
            "Step 4100: train loss 1.5804, val loss 1.7617\n",
            "Step 4200: train loss 1.5759, val loss 1.7589\n",
            "Step 4300: train loss 1.5699, val loss 1.7540\n",
            "Step 4400: train loss 1.5693, val loss 1.7441\n",
            "Step 4500: train loss 1.5577, val loss 1.7283\n",
            "Step 4600: train loss 1.5527, val loss 1.7513\n",
            "Step 4700: train loss 1.5512, val loss 1.7426\n",
            "Step 4800: train loss 1.5465, val loss 1.7367\n",
            "Step 4900: train loss 1.5490, val loss 1.7419\n",
            "Training completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This slightly improves the loss, and generates slightly more interpretable output."
      ],
      "metadata": {
        "id": "DGlytOspSODY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context = torch.zeros((1, 1), dtype=torch.long, device=config['device'])\n",
        "generated = model.generate(context, max_new_tokens=2000)\n",
        "print(parse_output(generated))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjx6XyMXRIot",
        "outputId": "ea38b3d6-3bd8-413a-f6ca-77e821999fdd"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "I enemy, yields his brothed to to medy?\n",
            "\n",
            "BUSTOHAS:\n",
            "Yet are heriks, I perpuate see: That entime,\n",
            "Even, sir, what I last I speak Stand him boy?\n",
            "\n",
            "CLARDIO:\n",
            "\n",
            "Butife?\n",
            "\n",
            "Nurse:\n",
            "Hence is four sinuce heir, how woundels, thought\n",
            "You by natural the best noble seen.\n",
            "\n",
            "LADWONTES:\n",
            "Go convilasting, my thy was be venol not the is\n",
            "\n",
            "LADY ANer to we the amagaret to my lords any,\n",
            "And sweetn From thyself you wound the bick.\n",
            "\n",
            "CAPULEY:\n",
            "Yor a your of her.\n",
            "\n",
            "QUEEN MARGARET:\n",
            "Gal, I'll you my my faice on will bitt Barke\n",
            "Trews buy angerad her, behold, count\n",
            "Where she barws thou guest up white bhose,\n",
            "Hath arreted in their moune with state with is\n",
            "With againster, forsorce at his armounted that her from\n",
            "The for of do graciour repest in their once\n",
            "Marggele, see, here of their that other,\n",
            "How 'my father, nor minds.\n",
            "\n",
            "Provost:\n",
            "Confersst it to imans; no is disciton.\n",
            "\n",
            "Servixd OF AUMP Spritizer:\n",
            "I she is give of is retacher from worship,\n",
            "Strudg dyst accuse her we angal her offf\n",
            "Both caught all saw if horse,\n",
            "To me hourt can quinests us.\n",
            "\n",
            "God Secuty.\n",
            "\n",
            "EXETER:\n",
            "Vo! wear she shall year ignify aning,\n",
            "That their had soo froment that I will\n",
            "For or excle your this he breed were of muiciush.\n",
            "I wilt have thy mercay, sweet more twic?\n",
            "\n",
            "MENENES:\n",
            "Not no cergerance to well up mine,\n",
            "Or Chupt evel in but itsfuil,\n",
            "How a havel the where morason's her.\n",
            "\n",
            "ROMEONES:\n",
            "I shadl a a ne her, hell your her,\n",
            "Some wakerd and enle's your swears:\n",
            "Such meet.\n",
            "\n",
            "QUEEN MAGO:\n",
            "Here medseral she termsulles; and what ifjuced as\n",
            "A qulamplantate yer this quenity.\n",
            "\n",
            "ARCHINCUS:\n",
            "My damor worse:\n",
            "For I have I wericed in it i' pale!\n",
            "\n",
            "Secoce:\n",
            "Faelst,\n",
            "She peasul-poiticeding, I the stick, you shall\n",
            "And Frared griesting on me\n",
            "My carce the begging campet, Clifffram make the\n",
            "confer likes true, Gest but of Clared.\n",
            "\n",
            "ROMEO:\n",
            "What unto the art agaluenst.\n",
            "\n",
            "DORCK:\n",
            "You not I knew dunk your death\n",
            "Your of the wanto stant take from\n",
            "Have an clean agroble, beimerful thee death furge\n",
            "Romands that our guest wereigng his hath he death;\n",
            "To be brother king, I 'lad iff your face\n",
            "Tha\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Increasing the model size: `n_embd` from `64` to `128`, `n_layer` from `4` to `8`. The model takes much longer to train (~10 min)."
      ],
      "metadata": {
        "id": "6XjR3gvOUX8F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    'batch_size' : 16, # no. of independent sequences to process in parallel\n",
        "    'block_size' : 256, # max no. of tokens to use for context in predictions\n",
        "    'max_iters' : 5000,\n",
        "    'eval_iters' : 200,\n",
        "    'eval_interval' : 100,\n",
        "    'learning_rate' : 1e-3,\n",
        "    'n_embd' : 128,\n",
        "    'n_head' : 4,\n",
        "    'n_layer' : 8,\n",
        "    'dropout' : 0.0,\n",
        "    'device' : 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "    'seed': 18916\n",
        "}\n",
        "\n",
        "torch.manual_seed(config['seed'])\n",
        "model, optimizer = get_model_and_optimizer(config, vocab_size)\n",
        "train(model, optimizer, config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VL9tt2yLSiXk",
        "outputId": "166be3a9-d56f-42d4-e58e-3d5756bbea51"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0: train loss 4.3153, val loss 4.3099\n",
            "Step 100: train loss 2.5195, val loss 2.5239\n",
            "Step 200: train loss 2.4634, val loss 2.4792\n",
            "Step 300: train loss 2.4036, val loss 2.4231\n",
            "Step 400: train loss 2.2503, val loss 2.2721\n",
            "Step 500: train loss 2.1366, val loss 2.1719\n",
            "Step 600: train loss 2.0100, val loss 2.0822\n",
            "Step 700: train loss 1.9273, val loss 2.0185\n",
            "Step 800: train loss 1.8418, val loss 1.9619\n",
            "Step 900: train loss 1.7773, val loss 1.9186\n",
            "Step 1000: train loss 1.7268, val loss 1.8688\n",
            "Step 1100: train loss 1.6749, val loss 1.8314\n",
            "Step 1200: train loss 1.6374, val loss 1.8008\n",
            "Step 1300: train loss 1.6070, val loss 1.7752\n",
            "Step 1400: train loss 1.5767, val loss 1.7648\n",
            "Step 1500: train loss 1.5544, val loss 1.7426\n",
            "Step 1600: train loss 1.5224, val loss 1.7275\n",
            "Step 1700: train loss 1.5113, val loss 1.6978\n",
            "Step 1800: train loss 1.4859, val loss 1.6838\n",
            "Step 1900: train loss 1.4737, val loss 1.6817\n",
            "Step 2000: train loss 1.4561, val loss 1.6634\n",
            "Step 2100: train loss 1.4484, val loss 1.6480\n",
            "Step 2200: train loss 1.4364, val loss 1.6336\n",
            "Step 2300: train loss 1.4164, val loss 1.6365\n",
            "Step 2400: train loss 1.4065, val loss 1.6199\n",
            "Step 2500: train loss 1.3976, val loss 1.6189\n",
            "Step 2600: train loss 1.3877, val loss 1.6016\n",
            "Step 2700: train loss 1.3779, val loss 1.6054\n",
            "Step 2800: train loss 1.3678, val loss 1.5898\n",
            "Step 2900: train loss 1.3625, val loss 1.5940\n",
            "Step 3000: train loss 1.3513, val loss 1.5849\n",
            "Step 3100: train loss 1.3509, val loss 1.5866\n",
            "Step 3200: train loss 1.3381, val loss 1.5646\n",
            "Step 3300: train loss 1.3355, val loss 1.5819\n",
            "Step 3400: train loss 1.3265, val loss 1.5635\n",
            "Step 3500: train loss 1.3222, val loss 1.5622\n",
            "Step 3600: train loss 1.3126, val loss 1.5587\n",
            "Step 3700: train loss 1.3112, val loss 1.5617\n",
            "Step 3800: train loss 1.3030, val loss 1.5556\n",
            "Step 3900: train loss 1.2939, val loss 1.5572\n",
            "Step 4000: train loss 1.2932, val loss 1.5428\n",
            "Step 4100: train loss 1.2833, val loss 1.5589\n",
            "Step 4200: train loss 1.2793, val loss 1.5669\n",
            "Step 4300: train loss 1.2833, val loss 1.5541\n",
            "Step 4400: train loss 1.2722, val loss 1.5450\n",
            "Step 4500: train loss 1.2614, val loss 1.5492\n",
            "Step 4600: train loss 1.2662, val loss 1.5478\n",
            "Step 4700: train loss 1.2591, val loss 1.5475\n",
            "Step 4800: train loss 1.2561, val loss 1.5516\n",
            "Step 4900: train loss 1.2507, val loss 1.5409\n",
            "Training completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss is much lower but the model takes much longer to generate a response. The response also better resembles Shakespeare. However, we may be overfitting to this training set."
      ],
      "metadata": {
        "id": "-5dDPalNU8It"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context = torch.zeros((1, 1), dtype=torch.long, device=config['device'])\n",
        "generated = model.generate(context, max_new_tokens=2000)\n",
        "print(parse_output(generated))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "be7B1mApU6c1",
        "outputId": "7b3bd9e9-978a-4071-f996-2401e5a05472"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Gentle-main; which brow'd! I will tought, the foul friar; and\n",
            "his mane-puase semblance, he must all of it.\n",
            "\n",
            "GLOUCESTER:\n",
            "You say this you mistre, I have foried\n",
            "itself to the good suits out recity.\n",
            "\n",
            "KOME EORD OVERD:\n",
            "\n",
            "NORTHUMBERLAND:\n",
            "I will plen my issue.\n",
            "\n",
            "LUCENTIO:\n",
            "Thou art last you may slaves be venominy.\n",
            "\n",
            "Musician:\n",
            "You entertainted amazened my mind.\n",
            "\n",
            "LUCIO:\n",
            "No, sir, nor before you humband?\n",
            "\n",
            "DUCHESS OF YORK:\n",
            "You shall for you must show't, and we call,--\n",
            "Boy! good my lord. How farest that it!\n",
            "\n",
            "DUKE OF YORK:\n",
            "Ay, in yea, wi come?\n",
            "\n",
            "DUCHESS OF YORK:\n",
            "\n",
            "\n",
            "KING RICHARD II:\n",
            "This that I know you welll againe with state.\n",
            "\n",
            "KING RICHARD II:\n",
            "I' the war too.\n",
            "\n",
            "KING EDWARD IV:\n",
            "'Twas sure is be? who dost about on man?\n",
            "\n",
            "BUCKINGHAM:\n",
            "I are groan, my mirth grave, all my groan,\n",
            "Harry, I comfort, and myself in have month.\n",
            "\n",
            "ROMIO:\n",
            "Tell him for all the better to didst all any that shame.\n",
            "If it thou sister'st but sad-worn in this diad's pace, and\n",
            "my young most to langually against\n",
            "that if wory goldeness-or actions now\n",
            "in any rose, stand whom on our mouth dreams when\n",
            "to ignorant in our affice, siving ours arms, bring\n",
            "on an courtesy: more, citizens be than doors, I know home thee\n",
            "are to furry yell. Speak; but his simprisons, and comparance: indeeds.\n",
            "\n",
            "LUCIO:\n",
            "What's tears?\n",
            "\n",
            "JULIET:\n",
            "Doth, things men stand her eyes, because misle; for breast\n",
            "compout in him! All yet the phoose report\n",
            "and enlighten-make, which adversarcise\n",
            "That: fie, be madam as sweeter's honour.\n",
            "\n",
            "Nurse:\n",
            "'just my father's kins: he may be be this,\n",
            "As his cheek'd words, woman: as he have!'\n",
            "\n",
            "Ghoster:\n",
            "Then, that is's itcup, talk'd on leave lorging?\n",
            "Giver me: if the Leontes; whose issue\n",
            "Of my maider masters caree\n",
            "Out of judge again, life, and go know\n",
            "Young end Polixred, Genten, and Claudio;\n",
            "O seeming is out?--as your punish, i'\n",
            "the ights not: if you camilliged,\n",
            "I' the good to run of my bastard.\n",
            "\n",
            "Second Montague again: your musffer'd, and priest, the weed\n",
            "I bear me for by.\n",
            "\n",
            "CLARENCE:\n",
            "My grief, when mirth. What, know let's and thy daughter,\n",
            "M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generating with a prompt"
      ],
      "metadata": {
        "id": "Ipdwf1QjVQYf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"First Citizen:\"\n",
        "context = torch.tensor([encode(prompt)], dtype=torch.long, device=config['device'])\n",
        "generated = model.generate(context, max_new_tokens=2000)\n",
        "print(parse_output(generated))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfM5mKZ-VPs2",
        "outputId": "16ff7ffd-ddc6-4541-c4fa-1aca1b4c93b3"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Comfort, sir, to think, unkner here come.\n",
            "\n",
            "Second Murderer:\n",
            "Be sit off:\n",
            "Ere he intend is in villain, if you cannot too mine!\n",
            "\n",
            "MONTAGUE:\n",
            "I players, then, bery on all is now:\n",
            "What's, and season him?\n",
            "\n",
            "Shepherd:\n",
            "Which Mifles, and keep thou find the man\n",
            "To put thy salicate this; then but do make some myself!\n",
            "\n",
            "PAULINA:\n",
            "O, thy queen!\n",
            "I should say, and mine revenge: I will promise thou\n",
            "be to shins: if thou refuse our birds, flesh,\n",
            "Thou art art well as thy Clifford,\n",
            "Till unstand thy companied! If you king to him.\n",
            "Thou loving from his darefic broke his\n",
            "Widoward's glassing actions and walk'd of him\n",
            "This angainst indeed-though of them,\n",
            "And womed thou he not with disdain.\n",
            "\n",
            "KING RICHARD III:\n",
            "How, more indeed?\n",
            "\n",
            "Second to Roman.\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "I have much rather when the rest, adieu.\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "Go to make them, for thou hast his mourn.\n",
            "\n",
            "KING RICHARD II:\n",
            "Thou doest me, for she stands upt man o'er.\n",
            "\n",
            "Keeper:\n",
            "Either, but not bosour his rapired!\n",
            "\n",
            "KING HENRY VI:\n",
            "Perhaps, 'tis he imagine with gross;\n",
            "I say not to be, this name on mine: as gase you,\n",
            "Whose worthy too committter: then the king\n",
            "The Turrison bart up hie! and there-good\n",
            "Gentlel and they hus arm,\n",
            "And when mother, tay, where more in arms,\n",
            "And tell thee thou less, the sed-wifing\n",
            "Radiers, though and he speak: though mighty, I\n",
            "world thou lovomity to do the king wide, opposted bust once,\n",
            "Unduched of the chide the pour mighty; and thou they may'st.\n",
            "\n",
            "KING EDWARD IV:\n",
            "There's youngeance, no more I challed,\n",
            "You must this disposing 'exasiance,\n",
            "Provoke and gived, here comfort commanded.\n",
            ".\n",
            "\n",
            "KING EDWARD IV:\n",
            "Pray, yourself, blustery; 'tis not me;\n",
            "Sir, pupon in justice, and what your abused\n",
            "The tick-his your friends; this is groan reign,\n",
            "Witness, to because a country man, you say.\n",
            "\n",
            "KING EDWARD IV:\n",
            "Arms, I am sworn: the matter yoke draw\n",
            "Here command. Is and reasons with that condemn's?\n",
            "\n",
            "GLOUCESTER:\n",
            "\n",
            "KING EDWARD IV:\n",
            "The world your reasons 'gainst leave the jeenting,\n",
            "Ene buys and tired and backeth yourself,\n",
            "Be he so advised watery thousa\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}